# üìò PubMedBERT Medical QA ‚Äì Fine-Tuning, Quantization & Explainability

This project implements a full end-to-end pipeline for biomedical question answering using **PubMedBERT**, including:

- Dataset preprocessing
- Model fine-tuning
- **8-bit quantization** for CPU-friendly inference
- Multiple explainability techniques (Attention, Perturbation, Integrated Gradients)
- A complete **Streamlit application**
- Evaluation metrics

This README summarizes the methodology, results, and instructions to run the system.

---

## üìö 1. Project Overview

The task involves fine-tuning a transformer model on the **PubMedQA (PQA-labeled)** dataset to answer biomedical _Yes/No/Maybe_ questions, followed by building a quantized and explainable inference system.

### **Final Deliverables**

- **fp32 fine-tuned model**
- **8-bit quantized model**
- **Inference + Explainability Streamlit UI**
- **Evaluation scripts**

---

## üìä 2. Dataset

**Dataset:** `pubmed_qa`, configuration **pqa_labeled**  
**Samples:** 1,000 labeled  
**Labels:** `"yes"`, `"no"`, `"maybe"`

### **Each example contains:**

- `question` ‚Äî biomedical question
- `context` ‚Äî symptoms or background
- `final_decision` ‚Äî gold label

### **Preprocessing**

Each sample is formatted as:
QUESTION: {question}.
CONTEXT: {context}

### **Label Encoding**

- `no` ‚Üí **0**
- `yes` ‚Üí **1**
- `maybe` ‚Üí **2**

All preprocessing uses the **datasets** library.

---

## üß† 3. Model

### **Base Model**

**PubMedBERT (uncased, abstracts only)**

- Specialized in biomedical text
- Excellent domain adaptation for medical QA

### **Fine-Tuned Model**

- 3-way **sequence classification**
- Trained on PubMedQA-lite
- Trained **on google collab GPU** for faster training, then downloaded to local usage on CPU

---

## üèãÔ∏è 4. Fine-Tuning

| Component  | Value                  |
| ---------- | ---------------------- |
| Model      | PubMedBERT             |
| Task       | 3-class classification |
| Optimizer  | AdamW                  |
| Epochs     | 3‚Äì5                    |
| Max Length | 256‚Äì384                |
| Batch Size | ~8 (CPU-dependent)     |

The training script performs preprocessing ‚Üí training ‚Üí saving.

### **Model Outputs**

- `pubmedbert_merged/` ‚Üí full-precision **fp32 model**
- `pubmedbert_8bit/` ‚Üí **INT8 quantized model**

---

## ‚ö° 5. Quantization (INT8)

Quantization uses **BitsandBytes 8-bit weight-only mode**:

```python
AutoModelForSequenceClassification.from_pretrained(
    MODEL_DIR,
    load_in_8bit=True,
    device_map="cpu"
)

```

## ‚ö° Benefits of INT8 Quantization

### **Comparison Table**

| Property       | FP32    | INT8            |
| -------------- | ------- | --------------- |
| **Model Size** | ~418 MB | ~128 MB         |
| **RAM Usage**  | ~900 MB | ~600‚Äì700 MB     |
| **Speed**      | Slow    | **2‚Äì3√ó faster** |

### **Measured Performance**

- **Inference latency:** ~0.97 seconds per sample
- **Weighted F1:** ~0.60
- **RSS memory:** ~685 MB

---

## üß© 6. Explainability Methods

Three independent explainability techniques are implemented:

### **1Ô∏è‚É£ Perturbation-based Token Importance**

- Mask each token
- Re-run the model
- Measure probability drop
- **Larger drop = more important token**

### **2Ô∏è‚É£ Attention Visualization**

- Extract attention from last layer
- Display **CLS attention bar chart**
- Display **token‚Äìtoken attention heatmap**

### **3Ô∏è‚É£ Integrated Gradients (Captum)**

- Requires the **fp32 model**
- Computes attribution scores from embeddings
- Shows how each token influences the prediction

---

## üñ•Ô∏è 7. Streamlit Application

Located at:

app.py

### **Features**

- Two required user inputs:
  - **Medical question**
  - **Symptoms / context**
- Model prediction (**Yes / No / Maybe**)
- Probability visualization (**Plotly**)
- Explainability:
  - Perturbation token importance
  - Attention heatmap + CLS-attention
  - Integrated Gradients
- Robust error handling
- CPU-optimized execution

---

## üìà 8. Evaluation

The script:

evaluate_inference.py

Measures:

- Prediction latency
- Weighted F1 on PubMedQA test split
- Memory usage

### **Example Output**

Avg latency per sample (s): 0.97\
F1 weighted: 0.5997\
RSS (MB): 685.3

---

## ‚ñ∂Ô∏è 10. How to Run

### **Install dependencies**

```bash
pip install -r requirements.txt
```

### Run Streamlit app

```bash
streamlit run app.py
```
